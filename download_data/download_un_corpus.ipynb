{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "168fa9b7-ce99-4517-8029-286723bae7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from urllib import parse\n",
    "from urllib.request import build_opener, install_opener, urlretrieve\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af52de0-1365-42ca-b6af-5cc0235c547e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_list = ['zh', 'en', 'fr', 'es', 'ru', 'ar']\n",
    "root_url = 'https://www.un.org/'\n",
    "url_status = {}\n",
    "link_pattern = r'<a href=\"((?:https?://[^/]*?\\.un\\.org)?/[^\"]*?)\"'\n",
    "media_format_list = ['avi', 'wmv', 'mpeg', 'mp4', 'mov', 'mkv', 'flv', 'f4v', 'm4v', 'rmvb', 'rm', '3gp', 'dat', 'ts',\n",
    "                     'mts', 'vob', 'bmp', 'jpg', 'png', 'tiff', 'gif', 'pcx', 'tga', 'exif', 'fpx', 'svg', 'psd', 'cdr',\n",
    "                     'pcd', 'dxf', 'ufo', 'eps', 'ai', 'raw', 'wmf', 'mp3', 'aiff', 'aac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556e2f51-19ac-4917-bbfc-f0a17d2e2cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url中含该list中的字符串则不处理\n",
    "excluded_url_pattern_list = ['search?', 'download?', 'subscribe?', 'system/403?', 'sustainabledevelopment.un.org/',\n",
    "                             'https://www.un.org/unispal/documents/?']\n",
    "# url符合pattern则做替换\n",
    "url_clean_pattern_list = [(r'^http://\\s*', 'https://'), (r'\\r?\\n', ''), (r'\\s*#.*', ''), (r'(?<=\\.pdf)&.*', ''),\n",
    "                          (r'(?<=asp)\\?.*', ''), (r'\\.un\\.org/../', '.un.org/'), (r'/[^./]*?/../', '/'),\n",
    "                          (r'\\s*[|/]$', ''), (r'https://(.*?\\.un\\.org)//\\1/', r'https://\\1/'), (' ', '%20')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3873c42-044c-48d4-b30c-8eaaec74df04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def escape(c):\n",
    "    # Windows下文件名无法包含以下几个字符，需要转义\n",
    "    return c.replace('?', '_QMARK_').replace(':', '_COLON_').replace('|', '_PIPE_').replace('/', '_SLASH_')\\\n",
    "        .replace('*', '_STAR_').replace('\"', '_QT_').replace('\\\\', '_BS_').replace('<', '_LT_').replace('>', '_GT_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35b1687-3e65-4f12-a37c-1b38221b2edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    global url_status\n",
    "    header = {\n",
    "        'user-agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/96.0.4664.110 Safari/537.36 \"\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(url, headers=header, timeout=30)\n",
    "        html = resp.content.decode('utf8')\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        print(\"%s: %s\" % (time.strftime('%Y-%m-%d %H:%M:%S'), str(e)))\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31900eca-1333-4cfe-9891-b9c30c50149d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_media(url):\n",
    "    if url.find('.') > 0:\n",
    "        tmp = re.sub(r'^.*\\.', '', url).lower()\n",
    "        if tmp in media_format_list:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_excluded_url_pattern(url):\n",
    "    if any(excluded_pattern in url for excluded_pattern in excluded_url_pattern_list):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def clean_url(url):\n",
    "    for pattern, repl in url_clean_pattern_list:\n",
    "        url = re.sub(pattern, repl, url)\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94e6e85-656e-4dce-aa7f-d19c98c7b9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_urls(curr_url, html):\n",
    "    urls = []\n",
    "    base_url = re.sub(r'(?<=\\.un\\.org)/.*', '', curr_url)\n",
    "    matched_urls = re.findall(link_pattern, html)\n",
    "    for url in matched_urls:\n",
    "        url = url.strip()\n",
    "        if url[0] == '/':\n",
    "            # 把相对路径改为绝对路径\n",
    "            url = base_url + url\n",
    "        if has_excluded_url_pattern(url):\n",
    "            # 如果url中包含某些字符串则不处理\n",
    "            continue\n",
    "        elif is_media(url):\n",
    "            # 如果是媒体格式的文件则不处理\n",
    "            continue\n",
    "        # 清洗url\n",
    "        url = clean_url(url)\n",
    "        if url not in urls:\n",
    "            urls.append(url)\n",
    "    return urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83a7699d-17f3-4d35-bbd8-29bc53694279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_md5(content):\n",
    "    md5 = hashlib.md5()\n",
    "    md5.update(content.encode('utf8'))\n",
    "    return md5.hexdigest()\n",
    "\n",
    "def make_content_item(index, url, status, content):\n",
    "    if content:\n",
    "        md5 = get_md5(content)\n",
    "    else:\n",
    "        md5 = None\n",
    "    ds_item = {'id': index, 'url': url, 'status': status, 'content': content, 'hash': md5, 'is_duplicate': 0}\n",
    "    return ds_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ccba8-9bda-48fb-923f-998faaf69f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    url_seed = load_dataset('hayesyang/un_corpus_seed', split='train')\n",
    "except:\n",
    "    initial_seed = {'id': [i for i in range(6)], 'url': [root_url + lang_list[i] for i in range(6)]}\n",
    "    url_seed = Dataset.from_dict(initial_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9329308c-a28c-492e-9061-80776e2a9621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/user/.cache/huggingface/datasets/hayesyang___parquet/hayesyang--un_corpus_content-8a63ccf80c52bb19/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    url_content = load_dataset('hayesyang/un_corpus_content', split='train')\n",
    "except:\n",
    "    url_content = Dataset.from_dict({'id':[], 'url':[], 'status': [], 'content': [], 'hash': [], 'is_duplicate': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c85f5f05-602a-49bb-9a9e-2726603182bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def push_to_hub(seed_set, content_set):\n",
    "    seed_set.push_to_hub('hayesyang/un_corpus_seed', token='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')\n",
    "    content_set.push_to_hub('hayesyang/un_corpus_content', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1627cd25-a5a2-4ea5-84c4-c7590e8eabdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process(seed_set, content_set):\n",
    "    start_index = len(content_set)\n",
    "    end_index = len(seed_set)\n",
    "    has_new = False\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(start_index, end_index):\n",
    "        url = seed_set[i]['url']\n",
    "        html = get_html(url)\n",
    "        if html:\n",
    "            new_content = make_content_item(i, url, 1, html)\n",
    "            if new_content['hash'] in content_set['hash']:\n",
    "                new_content['is_duplicate'] = 1\n",
    "            content_set.add_item(new_content)\n",
    "            \n",
    "            urls = parse_urls(url, html)\n",
    "            if len(urls) > 0:\n",
    "                for new_url in urls:\n",
    "                    if re.sub('/$', '', new_url) in seed_set['url']:\n",
    "                        continue\n",
    "                    seed_set.add_item({'id': len(seed_set), 'url': new_url})\n",
    "                    has_new = True\n",
    "            count += 1\n",
    "        else:\n",
    "            new_content = make_content_item(i, url, -1, None)\n",
    "            \n",
    "        if count % 200 == 0:\n",
    "            push_to_hub(seed_set, content_set)\n",
    "    \n",
    "    push_to_hub(seed_set, content_set)\n",
    "    return has_new, seed_set, content_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767ad8d-bf49-475e-b631-e0e64d11dab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-18 02:33:34: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 68/68 [00:00<00:00, 2731.92ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "Updating downloaded metadata with the new split.\n",
      "Pushing dataset shards to the dataset hub:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  20%|██        | 1/5 [00:00<00:02,  1.52ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  40%|████      | 2/5 [00:01<00:01,  1.60ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  60%|██████    | 3/5 [00:01<00:01,  1.55ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  80%|████████  | 4/5 [00:02<00:00,  1.40ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:03<00:00,  1.60ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub:  33%|███▎      | 1/3 [00:05<00:10,  5.30s/it]\n",
      "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  20%|██        | 1/5 [00:00<00:00,  4.27ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  40%|████      | 2/5 [00:00<00:00,  4.09ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  60%|██████    | 3/5 [00:00<00:00,  4.70ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00,  5.15ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\u001b[A\n",
      "Pushing dataset shards to the dataset hub:  67%|██████▋   | 2/3 [00:07<00:03,  3.45s/it]\n",
      "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  20%|██        | 1/5 [00:00<00:00,  4.39ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  40%|████      | 2/5 [00:00<00:01,  2.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  60%|██████    | 3/5 [00:00<00:00,  3.24ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  80%|████████  | 4/5 [00:01<00:00,  3.97ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:01<00:00,  3.24ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 3/3 [00:10<00:00,  3.51s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 3/3 [00:00<00:00,  4.08it/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "has_new_url, url_seed, url_content = process(url_seed, url_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c02e9-5bc5-4b98-854e-c40572e48a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while has_new_url:\n",
    "    has_new_url, url_seed, url_content = process(url_seed, url_content)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
