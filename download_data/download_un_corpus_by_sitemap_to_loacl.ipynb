{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "output_folder = './downloaded_websites'\n",
    "lang_list = ['zh', 'en', 'fr', 'es', 'ru', 'ar']\n",
    "error_url_save_path = \"./error_url.txt\"\n",
    "\n",
    "def save_error_url(url):\n",
    "    if os.path.isfile(error_url_save_path):\n",
    "        mode = 'a'\n",
    "    else:\n",
    "        mode = 'w'\n",
    "        \n",
    "    with open(error_url_save_path, mode) as f:\n",
    "         f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_html(url, retries=3, backoff_factor=0.5):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=retries, backoff_factor=backoff_factor)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"链接异常: {url} --- {e}\")\n",
    "        save_error_url(url)\n",
    "        return None\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_sitemap(sitemap_url):\n",
    "    html_text = get_html(sitemap_url)\n",
    "    if not html_text:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_text)\n",
    "    urls = [loc.get_text() for loc in soup.find_all('loc')]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_sitemap(sitemap_url):\n",
    "    html_text = get_html(sitemap_url)\n",
    "    if not html_text:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_text)\n",
    "    urls = [loc.get_text() for loc in soup.find_all('loc')]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_lang_url():\n",
    "    lang_with_urls = {}\n",
    "    \n",
    "    \n",
    "    print(\"Fetching all sitemap_urls...\")\n",
    "    for lang in lang_list:\n",
    "        folder_path = os.path.join(output_folder, lang)\n",
    "       \n",
    "        sitemap_urls = extract_urls_from_sitemap(f\"https://news.un.org/{lang}/sitemap.xml\")\n",
    "        is_lang_with_urls_exist = lang_with_urls.get(\"lang\",None)\n",
    "        \n",
    "        if not is_lang_with_urls_exist:\n",
    "            lang_with_urls[lang] = sitemap_urls\n",
    "        else:    \n",
    "            lang_with_urls[lang] += sitemap_urls\n",
    "    \n",
    "    print(\"Fetching all urls...\")\n",
    "    for lang in lang_with_urls:\n",
    "        urls = []\n",
    "        for lcos in lang_with_urls[lang]:\n",
    "            urls += extract_urls_from_sitemap(lcos)\n",
    "            \n",
    "        lang_with_urls[lang] += urls\n",
    "    \n",
    "    return lang_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./downloaded_websites\"\n",
    "\n",
    "def download_and_parse_page(tuple_parameter):\n",
    "    url, lang = tuple_parameter\n",
    "    \n",
    "    html_text = get_html(url)\n",
    "    \n",
    "    url_split = url.split(\"//\")\n",
    "    \n",
    "    filename =  url_split[1].replace(\"/\",\".\") if len(url_split) == 2 else \"no_prefix\" + url_split[0]\n",
    "    \n",
    "    save_path = os.path.join(output_folder,lang, filename + \".html\")\n",
    "    # print(save_path)\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from time import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "            \n",
    "    lang_with_urls = get_all_lang_url()\n",
    "    \n",
    "    # 遍历dataset中的所i有语言\n",
    "    for lang in lang_with_urls:\n",
    "        if not os.path.exists(output_folder + \"/\" + lang):\n",
    "            os.mkdir(output_folder + \"/\" + lang)\n",
    "            \n",
    "        urls = lang_with_urls[lang]\n",
    "        \n",
    "        # for url in urls:\n",
    "        #     download_and_parse_page((url,lang))\n",
    "        \n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count() * 8) as pool:\n",
    "            for _ in tqdm(pool.imap_unordered(download_and_parse_page, [(url, lang) for url in urls]), total=len(urls)):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all sitemap_urls...\n",
      "Fetching all urls...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
